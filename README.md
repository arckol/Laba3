# Инструменты анализа данных в примерах и задачах
Отчет по лабораторной работе #3 выполнил:
- Колин Арсений Витальевич
- РИ-210943

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | # | 20 |



## Цель работы
Познакомиться с программными средствами для создания системы машинного обучения и её интеграции в Unity.

##Скриншоты выполенения заданий:
https://disk.yandex.ru/d/lm4P7uW8I3i3Sg
## Задание 1

Скриншоты в файлообменнике

Выводы: ИИ учится выполнять нашу задачу.




## Задание 2
### Подробно описать каждую строку файла конфигурации нейронной сети. Самостоятельно найти информацию о компонентах Decision Requester, Behavior Parameters, добавленных сфере.


```py
behaviors:
  RollerBall: # указываем id агента
    trainer_type: ppo # режим обучения (Proximal Policy Optimization)
    hyperparameters:
      batch_size: 10 # количество опытов на каждой итерации
      buffer_size: 100 # количество опыта, которое нужно набрать перед обновлением модели
      learning_rate: 3.0e-4 # начальная скорость обучения
      beta: 5.0e-4 # сила регуляции энтропии, увеличивает случайность действий
      epsilon: 0.2 # порог расхождений между старой и новой политиками при обновлении
      lambd: 0.99 # параметр регуляции, насколько сильно агент полагается на текущий value estimate
      num_epoch: 3 # количество проходов через буфер опыта, при выполнении оптимизации
      learning_rate_schedule: linear # определяет как скорость обучения изменяется с течением времени
                                     # linear линейно уменьшает скорость
    network_settings:
      normalize: false # отключаем нормализацию входных данных
      hidden_units: 128 # количество нейронов в скрытых слоях сети
      num_layers: 2 # количество скрытых слоёв в сети
    reward_signals:
      extrinsic:
        gamma: 0.99 # коэффициент скидки для будущих вознаграждений
        strength: 1.0 # коэффициент на который умножается вознаграждение
    max_steps: 500000 # общее количество шагов, которые должны быть выполнены в среде до завершения обучения
    time_horizon: 64 # сколько опыта нужно собрать для каждого агента, прежде чем добавлять его в буфер
    summary_freq: 10000 # количество опыта, который необходимо собрать перед созданием и отображением статистики

```

Decision Requester - компонент, который запрашивает решение через постоянный промежуток времени.

Behavior Parameters - компонент, который определяет принятие объектом решений, в нём можно указать какой тип поведения будет использоваться: обученная модель или удалённый процесс обучения.


## Выводы
В этой лабораторной я научится на базовом уровне работать с MlAgents в unity.



